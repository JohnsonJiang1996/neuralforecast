# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/common.base_model.ipynb.

# %% auto 0
__all__ = []

# %% ../../nbs/common.base_model.ipynb 2
import warnings
from dataclasses import dataclass
from typing import Optional

import torch
import pytorch_lightning as pl

from neuralforecast.tsdataset import (
    _DistributedTimeSeriesDataModule,
    TimeSeriesDataModule,
    TimeSeriesDataset,
)

@dataclass
class DistributedConfig:
    partitions_path: str
    partitions_num: int
    num_nodes: int
    devices: int

# %% ../../nbs/common.base_model.ipynb 3
class _BaseModel(pl.LightningModule):
    def __init__(self):
        super().__init__()

    def _fit(
        self,
        dataset,
        batch_size,
        valid_batch_size=1024,
        val_size=0,
        test_size=0,
        random_seed=None,
        distributed_config: Optional[DistributedConfig] = None,
    ):
        # Check exogenous variables are contained in dataset
        temporal_cols = set(dataset.temporal_cols.tolist())
        static_cols = set(
            dataset.static_cols.tolist() if dataset.static_cols is not None else []
        )

        if len(set(self.hist_exog_list) - temporal_cols) > 0:
            raise Exception(
                f"{set(self.hist_exog_list) - temporal_cols} historical exogenous variables not found in input dataset"
            )
        if len(set(self.futr_exog_list) - temporal_cols) > 0:
            raise Exception(
                f"{set(self.futr_exog_list) - temporal_cols} future exogenous variables not found in input dataset"
            )
        if len(set(self.stat_exog_list) - static_cols) > 0:
            raise Exception(
                f"{set(self.stat_exog_list) - static_cols} static exogenous variables not found in input dataset"
            )

        # Restart random seed
        if random_seed is None:
            random_seed = self.random_seed
        torch.manual_seed(random_seed)

        self.val_size = val_size
        self.test_size = test_size
        is_local = isinstance(dataset, TimeSeriesDataset)
        if is_local:
            datamodule_constructor = TimeSeriesDataModule
        else:
            datamodule_constructor = _DistributedTimeSeriesDataModule
        datamodule = datamodule_constructor(
            dataset=dataset,
            batch_size=batch_size,
            valid_batch_size=valid_batch_size,
            num_workers=self.num_workers_loader,
            drop_last=self.drop_last_loader,
        )

        if self.val_check_steps > self.max_steps:
            warnings.warn(
                "val_check_steps is greater than max_steps, \
                    setting val_check_steps to max_steps"
            )
        val_check_interval = min(self.val_check_steps, self.max_steps)
        self.trainer_kwargs["val_check_interval"] = int(val_check_interval)
        self.trainer_kwargs["check_val_every_n_epoch"] = None

        if is_local:
            model = self
            trainer = pl.Trainer(**model.trainer_kwargs)
            trainer.fit(model, datamodule=datamodule)
            model.metrics = trainer.callback_metrics
            model.__dict__.pop('trainer', None)
        else:
            assert distributed_config is not None
            from pyspark.ml.torch.distributor import TorchDistributor

            def train_fn(
                model_cls,
                model_params,
                datamodule,
                trainer_kwargs,
                num_tasks,
                num_proc_per_task,
                val_size,
                test_size,
            ):
                import pytorch_lightning as pl

                # we instantiate here to avoid pickling large tensors (weights)
                model = model_cls(**model_params)
                model.val_size = val_size
                model.test_size = test_size
                for arg in ('devices', 'num_nodes'):
                    trainer_kwargs.pop(arg, None)
                trainer = pl.Trainer(
                    strategy="ddp",
                    use_distributed_sampler=False,  # to ensure our dataloaders are used as-is
                    num_nodes=num_tasks,
                    devices=num_proc_per_task,
                    **trainer_kwargs,
                )
                trainer.fit(model=model, datamodule=datamodule)
                model.metrics = trainer.callback_metrics
                model.__dict__.pop('trainer', None)
                return model

            def is_gpu_accelerator(accelerator):
                from pytorch_lightning.accelerators.cuda import CUDAAccelerator

                return (
                    accelerator == "gpu"
                    or isinstance(accelerator, CUDAAccelerator)
                    or (accelerator == "auto" and CUDAAccelerator.is_available())
                )

            local_mode = distributed_config.num_nodes == 1
            if local_mode:
                num_tasks = 1
                num_proc_per_task = distributed_config.devices
            else:
                num_tasks = distributed_config.devices * distributed_config.devices
                num_proc_per_task = 1  # number of GPUs per task
            num_proc = num_tasks * num_proc_per_task
            use_gpu = is_gpu_accelerator(self.trainer_kwargs["accelerator"])
            model = TorchDistributor(
                num_processes=num_proc,
                local_mode=local_mode,
                use_gpu=use_gpu,
            ).run(
                train_fn,
                model_cls=type(self),
                model_params=self.hparams,
                datamodule=datamodule,
                trainer_kwargs=self.trainer_kwargs,
                num_tasks=num_tasks,
                num_proc_per_task=num_proc_per_task,
                val_size=val_size,
                test_size=test_size,
            )
        return model
