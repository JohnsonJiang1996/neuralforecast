{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5c6594-e5e8-4966-8cb8-a3e6a9ed7d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp common._base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce0c950-2e03-4be1-95d4-a02409d8dba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7c2ba5-19ee-421e-9252-7224b03f5201",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from neuralforecast.tsdataset import (\n",
    "    _DistributedTimeSeriesDataModule,\n",
    "    TimeSeriesDataModule,\n",
    "    TimeSeriesDataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c40a64-8381-46a2-8cbb-70ec70ed7914",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class _BaseModel(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def _fit(\n",
    "        self,\n",
    "        dataset,\n",
    "        batch_size,\n",
    "        valid_batch_size=1024,\n",
    "        val_size=0,\n",
    "        test_size=0,\n",
    "        random_seed=None,\n",
    "    ):\n",
    "        # Check exogenous variables are contained in dataset\n",
    "        temporal_cols = set(dataset.temporal_cols.tolist())\n",
    "        static_cols = set(dataset.static_cols.tolist() if dataset.static_cols is not None else [])\n",
    "        \n",
    "        if len(set(self.hist_exog_list) - temporal_cols)>0:\n",
    "            raise Exception(f'{set(self.hist_exog_list) - temporal_cols} historical exogenous variables not found in input dataset')\n",
    "        if len(set(self.futr_exog_list) - temporal_cols)>0:\n",
    "            raise Exception(f'{set(self.futr_exog_list) - temporal_cols} future exogenous variables not found in input dataset')\n",
    "        if len(set(self.stat_exog_list) - static_cols)>0:\n",
    "            raise Exception(f'{set(self.stat_exog_list) - static_cols} static exogenous variables not found in input dataset')\n",
    "        \n",
    "        # Restart random seed\n",
    "        if random_seed is None:\n",
    "            random_seed = self.random_seed\n",
    "        torch.manual_seed(random_seed)\n",
    "        \n",
    "        self.val_size = val_size\n",
    "        self.test_size = test_size\n",
    "        is_local = isinstance(dataset, TimeSeriesDataset)\n",
    "        if is_local:\n",
    "            datamodule_constructor = TimeSeriesDataModule\n",
    "        else:\n",
    "            datamodule_constructor = _DistributedTimeSeriesDataModule\n",
    "        datamodule = datamodule_constructor(\n",
    "            dataset=dataset, \n",
    "            batch_size=batch_size,\n",
    "            valid_batch_size=valid_batch_size,\n",
    "            num_workers=self.num_workers_loader,\n",
    "            drop_last=self.drop_last_loader,\n",
    "        )\n",
    "\n",
    "        if self.val_check_steps > self.max_steps:\n",
    "            warnings.warn('val_check_steps is greater than max_steps, \\\n",
    "                    setting val_check_steps to max_steps')\n",
    "        val_check_interval = min(self.val_check_steps, self.max_steps)\n",
    "        self.trainer_kwargs['val_check_interval'] = int(val_check_interval)\n",
    "        self.trainer_kwargs['check_val_every_n_epoch'] = None\n",
    "\n",
    "        if is_local:\n",
    "            model = self            \n",
    "            trainer = pl.Trainer(**model.trainer_kwargs)\n",
    "            trainer.fit(model, datamodule=datamodule)\n",
    "        else:\n",
    "            from pyspark.ml.torch.distributor import TorchDistributor\n",
    "\n",
    "            def train_fn(model_cls, model_params, datamodule, trainer_kwargs):\n",
    "                import pytorch_lightning as pl\n",
    "\n",
    "                # we instantiate here to avoid pickling large tensors (weights)\n",
    "                model = model_cls(**model_params)\n",
    "                trainer = pl.Trainer(\n",
    "                    strategy='ddp',\n",
    "                    use_distributed_sampler=False,  # to ensure our dataloaders are used as-is\n",
    "                    devices=1,  # use only one GPU per task (total tasks = #gpus in cluster)\n",
    "                    **trainer_kwargs\n",
    "                )\n",
    "                trainer.fit(model=model, datamodule=datamodule)\n",
    "                return model, trainer\n",
    "\n",
    "            def is_gpu_accelerator(accelerator):\n",
    "                from pytorch_lightning.accelerators.cuda import CUDAAccelerator\n",
    "\n",
    "                return (\n",
    "                    accelerator == 'gpu'\n",
    "                    or isinstance(accelerator, CUDAAccelerator)\n",
    "                    or (accelerator == 'auto' and CUDAAccelerator.is_available())\n",
    "                )\n",
    "\n",
    "            devices = self.trainer_kwargs.pop('devices')\n",
    "            num_processes = self.trainer_kwargs['num_nodes'] * devices\n",
    "            use_gpu = is_gpu_accelerator(self.trainer_kwargs['accelerator'])\n",
    "            model, trainer = TorchDistributor(\n",
    "                num_processes=num_processes, local_mode=num_nodes == 1, use_gpu=use_gpu\n",
    "            ).run(train_fn, type(self), self.hparams, datamodule, self.trainer_kwargs)\n",
    "            del trainer['strategy'], model.trainer_kwargs['num_nodes']\n",
    "        model.trainer = trainer\n",
    "        return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
