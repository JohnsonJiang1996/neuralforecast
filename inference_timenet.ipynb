{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from neuralforecast.core import NeuralForecast\n",
    "from neuralforecast.losses.numpy import mae, mape, mase, rmse, smape\n",
    "from config_timenet import MODEL_LIST, load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(partitions_dataset, frequency):\n",
    "    urls = partitions_dataset['url'].values\n",
    "    df_list = []\n",
    "    for url in urls:\n",
    "        df_list.append(pd.read_parquet(url))\n",
    "    Y_df = pd.concat(df_list, axis=0).reset_index(drop=True)\n",
    "    Y_df['ds'] = pd.to_datetime(Y_df['ds']).dt.tz_localize(None)\n",
    "\n",
    "    if frequency == '30Minutely':\n",
    "        Y_df = Y_df.groupby('unique_id').tail(48*120).reset_index(drop=True)\n",
    "    if frequency == '10Minutely':\n",
    "        Y_df = Y_df.groupby('unique_id').tail(144*90).reset_index(drop=True)\n",
    "    if frequency == 'Minutely':\n",
    "        Y_df = Y_df.groupby('unique_id').tail(60*24*30).reset_index(drop=True)\n",
    "    return Y_df\n",
    "\n",
    "def run_inference(nf, Y_df, horizon):\n",
    "    Y_hat_df = nf.cross_validation(df=Y_df,\n",
    "                                   n_windows=1,\n",
    "                                   fit_models=False,\n",
    "                                   use_init_models=False).reset_index()\n",
    "    Y_hat_df = Y_hat_df.groupby('unique_id').tail(horizon)\n",
    "    print('nulls:', Y_hat_df['y'].isnull().sum())\n",
    "    return Y_hat_df\n",
    "\n",
    "def compute_losses(Y_hat_df, y_hat_col, dataset, subdataset, frequency):\n",
    "    mae_loss = mae(y=Y_hat_df['y'], y_hat=Y_hat_df[y_hat_col])\n",
    "    mape_loss = mape(y=Y_hat_df['y'], y_hat=Y_hat_df[y_hat_col])\n",
    "    rmse_loss = rmse(y=Y_hat_df['y'],y_hat=Y_hat_df[y_hat_col])\n",
    "    smape_loss = smape(y=Y_hat_df['y'], y_hat=Y_hat_df[y_hat_col])\n",
    "\n",
    "    row = pd.DataFrame({'dataset':[dataset], 'subdataset': [subdataset], 'frequency':frequency, 'mae': [mae_loss], 'mape':[mape_loss], 'rmse':[rmse_loss], 'smape':[smape_loss]})\n",
    "    df_results = pd.concat([df_results, row], ignore_index=True)\n",
    "    return df_results\n",
    "\n",
    "def compute_losses_by_ts(Y_hat_df, y_hat_col, model_name, dataset, subdataset, frequency):\n",
    "    mae_lambda = lambda x: mae(y=x['y'], y_hat=x[y_hat_col])\n",
    "    mape_lambda = lambda x: mape(y=x['y'], y_hat=x[y_hat_col])\n",
    "    rmse_lambda = lambda x: rmse(y=x['y'], y_hat=x[y_hat_col])\n",
    "    smape_lambda = lambda x: smape(y=x['y'], y_hat=x[y_hat_col])\n",
    "\n",
    "    df_metric_by_id = pd.DataFrame(columns=['unique_id', 'dataset', 'subdataset','metric', 'frequency', model_name])\n",
    "    for metric in [mae_lambda, mape_lambda, rmse_lambda, smape_lambda]:\n",
    "        Y_metric = Y_hat_df.groupby('unique_id').apply(metric)\n",
    "        if metric == mae_lambda:\n",
    "            metric = 'mae'\n",
    "        elif metric == mape_lambda:\n",
    "            metric = 'mape'\n",
    "        elif metric == rmse_lambda:\n",
    "            metric = 'rmse'\n",
    "        elif metric == smape_lambda:\n",
    "            metric = 'smape'\n",
    "        Y_metric = pd.DataFrame({'unique_id': Y_metric.index, 'dataset': dataset, 'subdataset': subdataset, 'metric': metric, 'frequency': frequency, 'NHITS': Y_metric.values})\n",
    "        df_metric_by_id = pd.concat([df_metric_by_id, Y_metric], ignore_index=True)\n",
    "    return df_metric_by_id"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yearly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "frequency = 'Yearly'\n",
    "source_dataset = 'timenet'\n",
    "model = 'nhits_30_1024_yearly'\n",
    "experiment_id = '20230626'\n",
    "horizon = 1\n",
    "\n",
    "# Run inference\n",
    "nf = NeuralForecast.load(path=\n",
    "        f'./results/stored_models/{source_dataset}/{model}/{experiment_id}/')\n",
    "\n",
    "parts_df = pd.read_csv('partitions_df.csv')\n",
    "parts_df = parts_df[parts_df['frequency'] == frequency]\n",
    "\n",
    "df_results = pd.DataFrame(columns = ['dataset', 'subdataset', 'frequency', 'mae', 'mape', 'rmse', 'smape'])\n",
    "datasets = parts_df['dataset'].unique()\n",
    "print('Datasets', datasets)\n",
    "for dataset in datasets:\n",
    "    parts_dataset = parts_df[parts_df['dataset'] == dataset]\n",
    "    subdatasets = parts_dataset['subdataset'].unique()\n",
    "    print('Subdatasets', subdatasets)\n",
    "    for subdataset in subdatasets:\n",
    "\n",
    "        if subdataset == 'M3':\n",
    "            freq = 'Y'\n",
    "        elif subdataset == 'M4':\n",
    "            freq = 'AS'\n",
    "        nf.freq = pd.tseries.frequencies.to_offset(freq)\n",
    "\n",
    "        subparts_dataset = parts_dataset[parts_dataset['subdataset'] == subdataset]\n",
    "\n",
    "        # Read Data\n",
    "        Y_df = read_data(partitions_dataset=subparts_dataset)\n",
    "\n",
    "        # Run inference \n",
    "        Y_hat_df = run_inference(nf=nf, Y_df=Y_df, horizon=horizon)\n",
    "        \n",
    "        # Compute metrics\n",
    "        df_results = compute_losses(Y_hat_df=Y_hat_df, y_hat_col='NHITS-median', dataset=dataset, subdataset=subdataset, frequency=frequency)\n",
    "        df_metric_by_id = compute_losses_by_ts(Y_hat_df=Y_hat_df, y_hat_col='NHITS-median', model_name='NHITS',\n",
    "                                               dataset=dataset, subdataset=subdataset, frequency=frequency)\n",
    "\n",
    "        df_results.to_csv('results_yearly.csv', index=False)\n",
    "        df_metric_by_id.to_parquet(f'./results/final/{dataset}_{subdataset}_{frequency}.parquet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quarterly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "frequency = 'Quarterly'\n",
    "source_dataset = 'timenet'\n",
    "model = 'nhits_30_1024_quarterly'\n",
    "experiment_id = '20230626'\n",
    "horizon = 4\n",
    "\n",
    "# Run inference\n",
    "nf = NeuralForecast.load(path=\n",
    "        f'./results/stored_models/{source_dataset}/{model}/{experiment_id}/')\n",
    "\n",
    "parts_df = pd.read_csv('partitions_df.csv')\n",
    "parts_df = parts_df[parts_df['frequency'] == frequency]\n",
    "\n",
    "df_results = pd.DataFrame(columns = ['dataset', 'subdataset', 'frequency', 'mae', 'mape', 'rmse', 'smape'])\n",
    "datasets = parts_df['dataset'].unique()\n",
    "print('Datasets', datasets)\n",
    "for dataset in datasets:\n",
    "    parts_dataset = parts_df[parts_df['dataset'] == dataset]\n",
    "    subdatasets = parts_dataset['subdataset'].unique()\n",
    "    print('Subdatasets', subdatasets)\n",
    "    for subdataset in subdatasets:\n",
    "        subparts_dataset = parts_dataset[parts_dataset['subdataset'] == subdataset]\n",
    "\n",
    "        if subdataset == 'M3':\n",
    "            freq = 'Q'\n",
    "        elif subdataset == 'M4':\n",
    "            freq = 'QS'\n",
    "        nf.freq = pd.tseries.frequencies.to_offset(freq)\n",
    "\n",
    "        # Read Data\n",
    "        Y_df = read_data(partitions_dataset=subparts_dataset)\n",
    "\n",
    "        # Run inference \n",
    "        Y_hat_df = run_inference(nf=nf, Y_df=Y_df, horizon=horizon)\n",
    "        \n",
    "        # Compute metrics\n",
    "        df_results = compute_losses(Y_hat_df=Y_hat_df, y_hat_col='NHITS-median', dataset=dataset, subdataset=subdataset, frequency=frequency)\n",
    "        df_metric_by_id = compute_losses_by_ts(Y_hat_df=Y_hat_df, y_hat_col='NHITS-median', model_name='NHITS',\n",
    "                                               dataset=dataset, subdataset=subdataset, frequency=frequency)\n",
    "\n",
    "        df_results.to_csv('results_quarterly.csv', index=False)\n",
    "        df_metric_by_id.to_parquet(f'./results/final/{dataset}_{subdataset}_{frequency}.parquet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monthly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "frequency = 'Monthly'\n",
    "source_dataset = 'timenet'\n",
    "model = 'nhits_30_1024_monthly'\n",
    "experiment_id = '20230626'\n",
    "horizon = 12\n",
    "\n",
    "# Run inference\n",
    "nf = NeuralForecast.load(path=\n",
    "        f'./results/stored_models/{source_dataset}/{model}/{experiment_id}/')\n",
    "\n",
    "parts_df = pd.read_csv('partitions_df.csv')\n",
    "parts_df = parts_df[parts_df['frequency'] == frequency]\n",
    "\n",
    "print('Removing Wiki')\n",
    "print('Partitions before: ', parts_df.shape)\n",
    "parts_df = parts_df[parts_df['subdataset'] != 'Mini']\n",
    "print('Partitions before: ', parts_df.shape)\n",
    "\n",
    "df_results = pd.DataFrame(columns = ['dataset', 'subdataset', 'frequency', 'mae', 'mape', 'rmse', 'smape'])\n",
    "datasets = parts_df['dataset'].unique()\n",
    "print('Datasets', datasets)\n",
    "for dataset in datasets:\n",
    "    parts_dataset = parts_df[parts_df['dataset'] == dataset]\n",
    "    subdatasets = parts_dataset['subdataset'].unique()\n",
    "    print('Subdatasets', subdatasets)\n",
    "    for subdataset in subdatasets:\n",
    "\n",
    "        if subdataset == 'M4':\n",
    "            freq = 'MS'\n",
    "        elif subdataset == 'M3':\n",
    "            freq = 'M'\n",
    "        elif subdataset == 'hospital':\n",
    "            freq = 'MS'\n",
    "        elif subdataset == 'car_parts':\n",
    "            freq = 'MS'\n",
    "        nf.freq = pd.tseries.frequencies.to_offset(freq)\n",
    "\n",
    "        subparts_dataset = parts_dataset[parts_dataset['subdataset'] == subdataset]\n",
    "\n",
    "        # Read Data\n",
    "        Y_df = read_data(partitions_dataset=subparts_dataset)\n",
    "\n",
    "        # Run inference \n",
    "        Y_hat_df = run_inference(nf=nf, Y_df=Y_df, horizon=horizon)\n",
    "        \n",
    "        # Compute metrics\n",
    "        df_results = compute_losses(Y_hat_df=Y_hat_df, y_hat_col='NHITS-median', dataset=dataset, subdataset=subdataset, frequency=frequency)        \n",
    "        df_metric_by_id = compute_losses_by_ts(Y_hat_df=Y_hat_df, y_hat_col='NHITS-median', model_name='NHITS',\n",
    "                                               dataset=dataset, subdataset=subdataset, frequency=frequency)\n",
    "\n",
    "        df_results.to_csv('results_monthly.csv', index=False)\n",
    "        df_metric_by_id.to_parquet(f'./results/final/{dataset}_{subdataset}_{frequency}.parquet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weekly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "frequency = 'Weekly'\n",
    "source_dataset = 'timenet'\n",
    "model = 'nhits_30_1024_weekly'\n",
    "experiment_id = '20230626'\n",
    "horizon = 1\n",
    "\n",
    "nf = NeuralForecast.load(path=\n",
    "        f'./results/stored_models/{source_dataset}/{model}/{experiment_id}/')\n",
    "\n",
    "parts_df = pd.read_csv('partitions_df.csv')\n",
    "parts_df = parts_df[parts_df['frequency'] == frequency]\n",
    "\n",
    "print('Removing Wiki')\n",
    "print('Partitions before: ', parts_df.shape)\n",
    "parts_df = parts_df[parts_df['subdataset'] != 'Mini']\n",
    "print('Partitions before: ', parts_df.shape)\n",
    "\n",
    "df_results = pd.DataFrame(columns = ['dataset', 'subdataset', 'frequency', 'mae', 'mape', 'rmse', 'smape'])\n",
    "datasets = parts_df['dataset'].unique()\n",
    "print('Datasets', datasets)\n",
    "for dataset in datasets:\n",
    "    parts_dataset = parts_df[parts_df['dataset'] == dataset]\n",
    "    subdatasets = parts_dataset['subdataset'].unique()\n",
    "    print('Subdatasets', subdatasets)\n",
    "    for subdataset in subdatasets:\n",
    "\n",
    "        if subdataset == 'ILI':\n",
    "            freq = 'W-TUE'\n",
    "        elif subdataset == 'electricity':\n",
    "            freq = 'W-SUN'\n",
    "        elif subdataset == 'nn5':\n",
    "            freq = 'W-MON'\n",
    "        nf.freq = pd.tseries.frequencies.to_offset(freq)\n",
    "\n",
    "        subparts_dataset = parts_dataset[parts_dataset['subdataset'] == subdataset]\n",
    "\n",
    "        # Read Data\n",
    "        Y_df = read_data(partitions_dataset=subparts_dataset)\n",
    "\n",
    "        # Run inference \n",
    "        Y_hat_df = run_inference(nf=nf, Y_df=Y_df, horizon=horizon)\n",
    "        \n",
    "        # Compute metrics\n",
    "        df_results = compute_losses(Y_hat_df=Y_hat_df, y_hat_col='NHITS-median', dataset=dataset, subdataset=subdataset, frequency=frequency) \n",
    "        df_metric_by_id = compute_losses_by_ts(Y_hat_df=Y_hat_df, y_hat_col='NHITS-median', model_name='NHITS',\n",
    "                                               dataset=dataset, subdataset=subdataset, frequency=frequency)\n",
    "\n",
    "        df_results.to_csv('results_weekly.csv', index=False)\n",
    "        df_metric_by_id.to_parquet(f'./results/final/{dataset}_{subdataset}_{frequency}.parquet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "frequency = 'Daily'\n",
    "source_dataset = 'timenet'\n",
    "model = 'nhits_30_1024_daily'\n",
    "experiment_id = '20230626'\n",
    "horizon = 7\n",
    "\n",
    "nf = NeuralForecast.load(path=\n",
    "        f'./results/stored_models/{source_dataset}/{model}/{experiment_id}/')\n",
    "\n",
    "# Run inference\n",
    "parts_df = pd.read_csv('partitions_df.csv')\n",
    "parts_df = parts_df[parts_df['frequency'] == frequency]\n",
    "print('Removing Wiki')\n",
    "print('Partitions before: ', parts_df.shape)\n",
    "parts_df = parts_df[parts_df['subdataset'] != 'Mini']\n",
    "print('Partitions before: ', parts_df.shape)\n",
    "\n",
    "df_results = pd.DataFrame(columns = ['dataset', 'subdataset', 'frequency', 'mae', 'mape', 'rmse', 'smape'])\n",
    "datasets = parts_df['dataset'].unique()\n",
    "print('Datasets', datasets)\n",
    "for dataset in datasets:\n",
    "    parts_dataset = parts_df[parts_df['dataset'] == dataset]\n",
    "    subdatasets = parts_dataset['subdataset'].unique()\n",
    "    print('Subdatasets', subdatasets)\n",
    "    for subdataset in subdatasets:\n",
    "\n",
    "        subparts_dataset = parts_dataset[parts_dataset['subdataset'] == subdataset]\n",
    "\n",
    "        # Read Data\n",
    "        Y_df = read_data(partitions_dataset=subparts_dataset)\n",
    "\n",
    "        # Run inference \n",
    "        Y_hat_df = run_inference(nf=nf, Y_df=Y_df, horizon=horizon)\n",
    "        \n",
    "        # Compute metrics\n",
    "        df_results = compute_losses(Y_hat_df=Y_hat_df, y_hat_col='NHITS-median', dataset=dataset, subdataset=subdataset, frequency=frequency)         \n",
    "        df_metric_by_id = compute_losses_by_ts(Y_hat_df=Y_hat_df, y_hat_col='NHITS-median', model_name='NHITS',\n",
    "                                               dataset=dataset, subdataset=subdataset, frequency=frequency)\n",
    "\n",
    "        df_results.to_csv('results_daily.csv', index=False)\n",
    "        df_metric_by_id.to_parquet(f'./results/final/{dataset}_{subdataset}_{frequency}.parquet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hourly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "frequency = 'Hourly'\n",
    "source_dataset = 'timenet'\n",
    "model = 'nhits_30_1024_hourly'\n",
    "experiment_id = '20230626'\n",
    "horizon = 24\n",
    "\n",
    "# Run inference\n",
    "nf = NeuralForecast.load(path=\n",
    "        f'./results/stored_models/{source_dataset}/{model}/{experiment_id}/')\n",
    "\n",
    "parts_df = pd.read_csv('partitions_df.csv')\n",
    "parts_df = parts_df[parts_df['frequency'] == frequency]\n",
    "\n",
    "df_results = pd.DataFrame(columns = ['dataset', 'subdataset', 'frequency', 'mae', 'mape', 'rmse', 'smape'])\n",
    "datasets = parts_df['dataset'].unique()\n",
    "print('Datasets', datasets)\n",
    "for dataset in datasets:\n",
    "    parts_dataset = parts_df[parts_df['dataset'] == dataset]\n",
    "    subdatasets = parts_dataset['subdataset'].unique()\n",
    "    print('Subdatasets', subdatasets)\n",
    "    for subdataset in subdatasets:\n",
    "        subparts_dataset = parts_dataset[parts_dataset['subdataset'] == subdataset]\n",
    "\n",
    "        # Read Data\n",
    "        Y_df = read_data(partitions_dataset=subparts_dataset)\n",
    "\n",
    "        # Run inference \n",
    "        Y_hat_df = run_inference(nf=nf, Y_df=Y_df, horizon=horizon)\n",
    "        \n",
    "        # Compute metrics\n",
    "        df_results = compute_losses(Y_hat_df=Y_hat_df, y_hat_col='NHITS-median', dataset=dataset, subdataset=subdataset, frequency=frequency)        \n",
    "        df_metric_by_id = compute_losses_by_ts(Y_hat_df=Y_hat_df, y_hat_col='NHITS-median', model_name='NHITS',\n",
    "                                               dataset=dataset, subdataset=subdataset, frequency=frequency)\n",
    "\n",
    "        df_results.to_csv('results_hourly.csv', index=False)\n",
    "        df_metric_by_id.to_parquet(f'./results/final/{dataset}_{subdataset}_{frequency}.parquet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 30 Minutely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "frequency = '30Minutely'\n",
    "source_dataset = 'timenet'\n",
    "model = 'nhits_30_1024_30minutely'\n",
    "experiment_id = '20230626'\n",
    "horizon = 48\n",
    "\n",
    "# Run inference\n",
    "nf = NeuralForecast.load(path=\n",
    "        f'./results/stored_models/{source_dataset}/{model}/{experiment_id}/')\n",
    "\n",
    "parts_df = pd.read_csv('partitions_df.csv')\n",
    "parts_df = parts_df[parts_df['frequency'] == frequency]\n",
    "\n",
    "df_results = pd.DataFrame(columns = ['dataset', 'subdataset', 'frequency', 'mae', 'mape', 'rmse', 'smape'])\n",
    "datasets = parts_df['dataset'].unique()\n",
    "print('Datasets', datasets)\n",
    "for dataset in datasets:\n",
    "    parts_dataset = parts_df[parts_df['dataset'] == dataset]\n",
    "    subdatasets = parts_dataset['subdataset'].unique()\n",
    "    print('Subdatasets', subdatasets)\n",
    "    for subdataset in subdatasets:\n",
    "\n",
    "        freq = '30T'\n",
    "        nf.freq = pd.tseries.frequencies.to_offset(freq)\n",
    "\n",
    "        subparts_dataset = parts_dataset[parts_dataset['subdataset'] == subdataset]\n",
    "\n",
    "        # Read Data\n",
    "        Y_df = read_data(partitions_dataset=subparts_dataset)\n",
    "\n",
    "        # Run inference \n",
    "        Y_hat_df = run_inference(nf=nf, Y_df=Y_df, horizon=horizon)\n",
    "        \n",
    "        # Compute metrics\n",
    "        df_results = compute_losses(Y_hat_df=Y_hat_df, y_hat_col='NHITS-median', dataset=dataset, subdataset=subdataset, frequency=frequency)            \n",
    "        df_metric_by_id = compute_losses_by_ts(Y_hat_df=Y_hat_df, y_hat_col='NHITS-median', model_name='NHITS',\n",
    "                                               dataset=dataset, subdataset=subdataset, frequency=frequency)\n",
    "\n",
    "        df_results.to_csv('results_30minutely.csv', index=False)\n",
    "        df_metric_by_id.to_parquet(f'./results/final/{dataset}_{subdataset}_{frequency}.parquet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15minutely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "frequency = '15Minutely'\n",
    "source_dataset = 'timenet'\n",
    "model = 'nhits_30_1024_15minutely'\n",
    "experiment_id = '20230626'\n",
    "horizon = 96\n",
    "\n",
    "# Run inference\n",
    "nf = NeuralForecast.load(path=\n",
    "        f'./results/stored_models/{source_dataset}/{model}/{experiment_id}/')\n",
    "\n",
    "parts_df = pd.read_csv('partitions_df.csv')\n",
    "parts_df = parts_df[parts_df['frequency'] == frequency]\n",
    "print('Removing ECL')\n",
    "print('Partitions before: ', parts_df.shape)\n",
    "parts_df = parts_df[parts_df['subdataset'] != 'ECL']\n",
    "print('Partitions before: ', parts_df.shape)\n",
    "\n",
    "df_results = pd.DataFrame(columns = ['dataset', 'subdataset', 'frequency', 'mae', 'mape', 'rmse', 'smape'])\n",
    "datasets = parts_df['dataset'].unique()\n",
    "print('Datasets', datasets)\n",
    "for dataset in datasets:\n",
    "    parts_dataset = parts_df[parts_df['dataset'] == dataset]\n",
    "    subdatasets = parts_dataset['subdataset'].unique()\n",
    "    print('Subdatasets', subdatasets)\n",
    "    for subdataset in subdatasets:\n",
    "\n",
    "        freq = '15T'\n",
    "        nf.freq = pd.tseries.frequencies.to_offset(freq)\n",
    "\n",
    "        subparts_dataset = parts_dataset[parts_dataset['subdataset'] == subdataset]\n",
    "\n",
    "        # Read Data\n",
    "        Y_df = read_data(partitions_dataset=subparts_dataset)\n",
    "\n",
    "        # Run inference \n",
    "        Y_hat_df = run_inference(nf=nf, Y_df=Y_df, horizon=horizon)\n",
    "        \n",
    "        # Compute metrics\n",
    "        df_results = compute_losses(Y_hat_df=Y_hat_df, y_hat_col='NHITS-median', dataset=dataset, subdataset=subdataset, frequency=frequency)        \n",
    "        df_metric_by_id = compute_losses_by_ts(Y_hat_df=Y_hat_df, y_hat_col='NHITS-median', model_name='NHITS',\n",
    "                                               dataset=dataset, subdataset=subdataset, frequency=frequency)\n",
    "\n",
    "        df_results.to_csv('results_15minutely.csv', index=False)\n",
    "        df_metric_by_id.to_parquet(f'./results/final/{dataset}_{subdataset}_{frequency}.parquet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10minutely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "frequency = '10Minutely'\n",
    "source_dataset = 'timenet'\n",
    "model = 'nhits_30_1024_10minutely'\n",
    "experiment_id = '20230626'\n",
    "horizon = 144\n",
    "\n",
    "# Run inference\n",
    "nf = NeuralForecast.load(path=\n",
    "        f'./results/stored_models/{source_dataset}/{model}/{experiment_id}/')\n",
    "\n",
    "parts_df = pd.read_csv('partitions_df.csv')\n",
    "parts_df = parts_df[parts_df['frequency'] == frequency]\n",
    "\n",
    "df_results = pd.DataFrame(columns = ['dataset', 'subdataset', 'frequency', 'mae', 'mape', 'rmse', 'smape'])\n",
    "datasets = parts_df['dataset'].unique()\n",
    "print('Datasets', datasets)\n",
    "for dataset in datasets:\n",
    "    parts_dataset = parts_df[parts_df['dataset'] == dataset]\n",
    "    subdatasets = parts_dataset['subdataset'].unique()\n",
    "    print('Subdatasets', subdatasets)\n",
    "    for subdataset in subdatasets:\n",
    "\n",
    "        freq = '10T'\n",
    "        nf.freq = pd.tseries.frequencies.to_offset(freq)\n",
    "\n",
    "        subparts_dataset = parts_dataset[parts_dataset['subdataset'] == subdataset]\n",
    "\n",
    "        # Read Data\n",
    "        Y_df = read_data(partitions_dataset=subparts_dataset)\n",
    "\n",
    "        # Run inference \n",
    "        Y_hat_df = run_inference(nf=nf, Y_df=Y_df, horizon=horizon)\n",
    "        \n",
    "        # Compute metrics\n",
    "        df_results = compute_losses(Y_hat_df=Y_hat_df, y_hat_col='NHITS-median', dataset=dataset, subdataset=subdataset, frequency=frequency)           \n",
    "        df_metric_by_id = compute_losses_by_ts(Y_hat_df=Y_hat_df, y_hat_col='NHITS-median', model_name='NHITS',\n",
    "                                               dataset=dataset, subdataset=subdataset, frequency=frequency)\n",
    "\n",
    "        df_results.to_csv('results_10minutely.csv', index=False)\n",
    "        df_metric_by_id.to_parquet(f'./results/final/{dataset}_{subdataset}_{frequency}.parquet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minutely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "frequency = 'Minutely'\n",
    "source_dataset = 'timenet'\n",
    "model = 'nhits_30_1024_minutely'\n",
    "experiment_id = '20230626'\n",
    "horizon = 60\n",
    "\n",
    "# Run inference\n",
    "nf = NeuralForecast.load(path=\n",
    "        f'./results/stored_models/{source_dataset}/{model}/{experiment_id}/')\n",
    "\n",
    "parts_df = pd.read_csv('partitions_df.csv')\n",
    "parts_df = parts_df[parts_df['frequency'] == frequency]\n",
    "\n",
    "df_results = pd.DataFrame(columns = ['dataset', 'subdataset', 'frequency', 'mae', 'mape', 'rmse', 'smape'])\n",
    "datasets = parts_df['dataset'].unique()\n",
    "print('Datasets', datasets)\n",
    "for dataset in datasets:\n",
    "    parts_dataset = parts_df[parts_df['dataset'] == dataset]\n",
    "    subdatasets = parts_dataset['subdataset'].unique()\n",
    "    print('Subdatasets', subdatasets)\n",
    "    for subdataset in subdatasets:\n",
    "\n",
    "        freq = 'T'\n",
    "        nf.freq = pd.tseries.frequencies.to_offset(freq)\n",
    "\n",
    "        subparts_dataset = parts_dataset[parts_dataset['subdataset'] == subdataset]\n",
    "\n",
    "        # Read Data\n",
    "        Y_df = read_data(partitions_dataset=subparts_dataset)\n",
    "\n",
    "        # Run inference \n",
    "        Y_hat_df = run_inference(nf=nf, Y_df=Y_df, horizon=horizon)\n",
    "        \n",
    "        # Compute metrics\n",
    "        df_results = compute_losses(Y_hat_df=Y_hat_df, y_hat_col='NHITS-median', dataset=dataset, subdataset=subdataset, frequency=frequency)        \n",
    "        df_metric_by_id = compute_losses_by_ts(Y_hat_df=Y_hat_df, y_hat_col='NHITS-median', model_name='NHITS',\n",
    "                                               dataset=dataset, subdataset=subdataset, frequency=frequency)\n",
    "\n",
    "        df_results.to_csv('results_minutely.csv', index=False)\n",
    "        df_metric_by_id.to_parquet(f'./results/final/{dataset}_{subdataset}_{frequency}.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuralforecast",
   "language": "python",
   "name": "neuralforecast"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
