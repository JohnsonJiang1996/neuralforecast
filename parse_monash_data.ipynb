{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from distutils.util import strtobool\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts the contents in a .tsf file into a dataframe and returns it along with other meta-data of the dataset: frequency, horizon, whether the dataset contains missing values and whether the series have equal lengths\n",
    "#\n",
    "# Parameters\n",
    "# full_file_path_and_name - complete .tsf file path\n",
    "# replace_missing_vals_with - a term to indicate the missing values in series in the returning dataframe\n",
    "# value_column_name - Any name that is preferred to have as the name of the column containing series values in the returning dataframe\n",
    "def convert_tsf_to_dataframe(\n",
    "    full_file_path_and_name,\n",
    "    replace_missing_vals_with=\"NaN\",\n",
    "    value_column_name=\"series_value\",\n",
    "):\n",
    "    col_names = []\n",
    "    col_types = []\n",
    "    all_data = {}\n",
    "    line_count = 0\n",
    "    frequency = None\n",
    "    forecast_horizon = None\n",
    "    contain_missing_values = None\n",
    "    contain_equal_length = None\n",
    "    found_data_tag = False\n",
    "    found_data_section = False\n",
    "    started_reading_data_section = False\n",
    "\n",
    "    with open(full_file_path_and_name, \"r\", encoding=\"cp1252\") as file:\n",
    "    #with open(full_file_path_and_name, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            # Strip white space from start/end of line\n",
    "            line = line.strip()\n",
    "\n",
    "            if line:\n",
    "                if line.startswith(\"@\"):  # Read meta-data\n",
    "                    if not line.startswith(\"@data\"):\n",
    "                        line_content = line.split(\" \")\n",
    "                        if line.startswith(\"@attribute\"):\n",
    "                            if (\n",
    "                                len(line_content) != 3\n",
    "                            ):  # Attributes have both name and type\n",
    "                                raise Exception(\"Invalid meta-data specification.\")\n",
    "\n",
    "                            col_names.append(line_content[1])\n",
    "                            col_types.append(line_content[2])\n",
    "                        else:\n",
    "                            if (\n",
    "                                len(line_content) != 2\n",
    "                            ):  # Other meta-data have only values\n",
    "                                raise Exception(\"Invalid meta-data specification.\")\n",
    "\n",
    "                            if line.startswith(\"@frequency\"):\n",
    "                                frequency = line_content[1]\n",
    "                            elif line.startswith(\"@horizon\"):\n",
    "                                forecast_horizon = int(line_content[1])\n",
    "                            elif line.startswith(\"@missing\"):\n",
    "                                contain_missing_values = bool(\n",
    "                                    strtobool(line_content[1])\n",
    "                                )\n",
    "                            elif line.startswith(\"@equallength\"):\n",
    "                                contain_equal_length = bool(strtobool(line_content[1]))\n",
    "\n",
    "                    else:\n",
    "                        if len(col_names) == 0:\n",
    "                            raise Exception(\n",
    "                                \"Missing attribute section. Attribute section must come before data.\"\n",
    "                            )\n",
    "\n",
    "                        found_data_tag = True\n",
    "                elif not line.startswith(\"#\"):\n",
    "                    if len(col_names) == 0:\n",
    "                        raise Exception(\n",
    "                            \"Missing attribute section. Attribute section must come before data.\"\n",
    "                        )\n",
    "                    elif not found_data_tag:\n",
    "                        raise Exception(\"Missing @data tag.\")\n",
    "                    else:\n",
    "                        if not started_reading_data_section:\n",
    "                            started_reading_data_section = True\n",
    "                            found_data_section = True\n",
    "                            all_series = []\n",
    "\n",
    "                            for col in col_names:\n",
    "                                all_data[col] = []\n",
    "\n",
    "                        full_info = line.split(\":\")\n",
    "\n",
    "                        if len(full_info) != (len(col_names) + 1):\n",
    "                            raise Exception(\"Missing attributes/values in series.\")\n",
    "\n",
    "                        series = full_info[len(full_info) - 1]\n",
    "                        series = series.split(\",\")\n",
    "\n",
    "                        if len(series) == 0:\n",
    "                            raise Exception(\n",
    "                                \"A given series should contains a set of comma separated numeric values. At least one numeric value should be there in a series. Missing values should be indicated with ? symbol\"\n",
    "                            )\n",
    "\n",
    "                        numeric_series = []\n",
    "\n",
    "                        for val in series:\n",
    "                            if val == \"?\":\n",
    "                                numeric_series.append(replace_missing_vals_with)\n",
    "                            else:\n",
    "                                numeric_series.append(float(val))\n",
    "\n",
    "                        if numeric_series.count(replace_missing_vals_with) == len(\n",
    "                            numeric_series\n",
    "                        ):\n",
    "                            raise Exception(\n",
    "                                \"All series values are missing. A given series should contains a set of comma separated numeric values. At least one numeric value should be there in a series.\"\n",
    "                            )\n",
    "\n",
    "                        all_series.append(pd.Series(numeric_series).array)\n",
    "\n",
    "                        for i in range(len(col_names)):\n",
    "                            att_val = None\n",
    "                            if col_types[i] == \"numeric\":\n",
    "                                att_val = int(full_info[i])\n",
    "                            elif col_types[i] == \"string\":\n",
    "                                att_val = str(full_info[i])\n",
    "                            elif col_types[i] == \"date\":\n",
    "                                att_val = datetime.strptime(\n",
    "                                    full_info[i], \"%Y-%m-%d %H-%M-%S\"\n",
    "                                )\n",
    "                            else:\n",
    "                                raise Exception(\n",
    "                                    \"Invalid attribute type.\"\n",
    "                                )  # Currently, the code supports only numeric, string and date types. Extend this as required.\n",
    "\n",
    "                            if att_val is None:\n",
    "                                raise Exception(\"Invalid attribute value.\")\n",
    "                            else:\n",
    "                                all_data[col_names[i]].append(att_val)\n",
    "\n",
    "                line_count = line_count + 1\n",
    "\n",
    "        if line_count == 0:\n",
    "            raise Exception(\"Empty file.\")\n",
    "        if len(col_names) == 0:\n",
    "            raise Exception(\"Missing attribute section.\")\n",
    "        if not found_data_section:\n",
    "            raise Exception(\"Missing series information under data section.\")\n",
    "\n",
    "        all_data[value_column_name] = all_series\n",
    "        loaded_data = pd.DataFrame(all_data)\n",
    "\n",
    "        return (\n",
    "            loaded_data,\n",
    "            frequency,\n",
    "            forecast_horizon,\n",
    "            contain_missing_values,\n",
    "            contain_equal_length,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _create_time_series(unique_id, start, periods, values, freq):\n",
    "#     print('start', start)\n",
    "#     assert 1<0, 'STOP'\n",
    "#     total_dates = pd.date_range(start=start, periods=periods, freq=freq)\n",
    "#     time_series = pd.DataFrame({'ts_name': unique_id, 'ds': total_dates, 'y': values})\n",
    "#     return time_series\n",
    "\n",
    "# def parse_data(loaded_data, frequency):\n",
    "#     loaded_data['count'] = loaded_data['series_value'].apply(lambda x: len(x))\n",
    "#     data = pd.concat([_create_time_series(row[1]['series_name'],\n",
    "#                                           row[1]['start_timestamp'],\n",
    "#                                           row[1]['count'],\n",
    "#                                           row[1]['series_value'],\n",
    "#                                           frequency) for row in loaded_data.iterrows()])\n",
    "#     return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_time_series(unique_id, periods, values, freq):\n",
    "    time_series = pd.DataFrame({'ts_name': unique_id, 'ds': range(len(values)), 'y': values})\n",
    "    return time_series\n",
    "\n",
    "def parse_data(loaded_data, frequency):\n",
    "    loaded_data['count'] = loaded_data['series_value'].apply(lambda x: len(x))\n",
    "    data = pd.concat([_create_time_series(row[1]['series_name'],\n",
    "                                          row[1]['count'],\n",
    "                                          row[1]['series_value'],\n",
    "                                          frequency) for row in loaded_data.iterrows()])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_dict = {'nn5_daily_dataset_without_missing_values': 'D',\n",
    "                  'nn5_weekly_dataset': 'W-MON',\n",
    "                  'london_smart_meters_dataset_without_missing_values': '30min', \n",
    "                  'car_parts_dataset_without_missing_values': 'MS',\n",
    "                  'covid_deaths_dataset':'D',\n",
    "                  'solar_10_minutes_dataset': '10min',\n",
    "                  'hospital_dataset': 'MS',\n",
    "                  'electricity_hourly_dataset': 'H',\n",
    "                  'electricity_weekly_dataset': 'W-SUN',\n",
    "                  'pedestrian_counts_dataset': 'H',\n",
    "                  'kdd_cup_2018_dataset_without_missing_values': 'H',\n",
    "                  'wind_farms_minutely_dataset_without_missing_values': 'T',\n",
    "                  'fred_md_dataset': 'MS',\n",
    "                  'sunspot_dataset_without_missing_values': 'D',\n",
    "                  'dominick_dataset': 'W',\n",
    "                  'kaggle_web_traffic_dataset_without_missing_values': 'D'}\n",
    "\n",
    "data_dict = {'nn5_daily_dataset_without_missing_values': 'nn5_daily',\n",
    "             'nn5_weekly_dataset': 'nn5_weekly',\n",
    "             'london_smart_meters_dataset_without_missing_values': 'london_smart_meters',\n",
    "             'car_parts_dataset_without_missing_values': 'car_parts',\n",
    "             'covid_deaths_dataset': 'covid_deaths',\n",
    "             'solar_10_minutes_dataset': 'solar_alabama',\n",
    "             'hospital_dataset': 'hospital',\n",
    "             'electricity_hourly_dataset': 'electricity_hourly',\n",
    "             'electricity_weekly_dataset': 'electricity_weekly',\n",
    "             'pedestrian_counts_dataset': 'pedestrian_counts',\n",
    "             'kdd_cup_2018_dataset_without_missing_values': 'kdd_cup_2018',\n",
    "             'wind_farms_minutely_dataset_without_missing_values': 'wind_farms',\n",
    "             'fred_md_dataset': 'fred_md',\n",
    "             'sunspot_dataset_without_missing_values': 'sunspot',\n",
    "             'dominick_dataset': 'dominick',\n",
    "             'kaggle_web_traffic_dataset_without_missing_values': 'web_traffic'}\n",
    "\n",
    "# datasets = ['nn5_daily_dataset_without_missing_values',\n",
    "#             'nn5_weekly_dataset',\n",
    "#             'london_smart_meters_dataset_without_missing_values',\n",
    "#             'car_parts_dataset_without_missing_values',\n",
    "#             'covid_deaths_dataset',\n",
    "#             'solar_10_minutes_dataset',\n",
    "#             'hospital_dataset',\n",
    "#             'electricity_hourly_dataset',\n",
    "#             'electricity_weekly_dataset',\n",
    "#             'pedestrian_counts_dataset',\n",
    "#             'kdd_cup_2018_dataset_without_missing_values']\n",
    "#datasets =   ['wind_farms_minutely_dataset_without_missing_values']\n",
    "#datasets = ['fred_md_dataset']\n",
    "#datasets = ['sunspot_dataset_without_missing_values']\n",
    "datasets = ['kaggle_web_traffic_dataset_without_missing_values']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:  kaggle_web_traffic_dataset_without_missing_values\n"
     ]
    }
   ],
   "source": [
    "data_list = []\n",
    "for data_name in datasets:\n",
    "    print('data: ', data_name)\n",
    "    loaded_data, frequency, forecast_horizon, contain_missing_values, contain_equal_length = convert_tsf_to_dataframe(f\"data_monash/{data_name}.tsf\")\n",
    "    data = parse_data(loaded_data=loaded_data, frequency=frequency_dict[data_name])\n",
    "    data['dataset'] = data_dict[data_name]\n",
    "    data['frequency'] = frequency_dict[data_name]\n",
    "    data_list.append(data)\n",
    "complete_data = pd.concat(data_list).reset_index(drop=True)\n",
    "complete_data['to_hash'] = complete_data['dataset'] + '_' + complete_data['ts_name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## timetnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_data['unique_id'] = complete_data['to_hash'].apply(lambda x: hashlib.sha1(x.encode()).hexdigest())\n",
    "complete_data = complete_data[['unique_id', 'ds', 'y', 'dataset', 'ts_name', 'frequency']]\n",
    "complete_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_data.to_parquet('monash_2.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 1<0, 'STOP'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loaded_data, frequency, forecast_horizon, contain_missing_values, contain_equal_length = convert_tsf_to_dataframe(\"data_monash/nn5_daily_dataset_without_missing_values.tsf\")            # DONE\n",
    "#loaded_data, frequency, forecast_horizon, contain_missing_values, contain_equal_length = convert_tsf_to_dataframe(\"data_monash/nn5_weekly_dataset.tsf\")                                  # DONE\n",
    "#loaded_data, frequency, forecast_horizon, contain_missing_values, contain_equal_length = convert_tsf_to_dataframe(\"data_monash/london_smart_meters_dataset_without_missing_values.tsf\")  # DONE\n",
    "#loaded_data, frequency, forecast_horizon, contain_missing_values, contain_equal_length = convert_tsf_to_dataframe(\"data_monash/car_parts_dataset_without_missing_values.tsf\")            # DONE\n",
    "#loaded_data, frequency, forecast_horizon, contain_missing_values, contain_equal_length = convert_tsf_to_dataframe(\"data_monash/covid_deaths_dataset.tsf\")                                # DONE\n",
    "#loaded_data, frequency, forecast_horizon, contain_missing_values, contain_equal_length = convert_tsf_to_dataframe(\"data_monash/solar_10_minutes_dataset.tsf\")                            # DONE\n",
    "#loaded_data, frequency, forecast_horizon, contain_missing_values, contain_equal_length = convert_tsf_to_dataframe(\"data_monash/hospital_dataset.tsf\")                                    # DONE\n",
    "#loaded_data, frequency, forecast_horizon, contain_missing_values, contain_equal_length = convert_tsf_to_dataframe(\"data_monash/electricity_hourly_dataset.tsf\")                          # DONE\n",
    "#loaded_data, frequency, forecast_horizon, contain_missing_values, contain_equal_length = convert_tsf_to_dataframe(\"data_monash/wind_farms_minutely_dataset_without_missing_values.tsf\")  # DONE\n",
    "#loaded_data, frequency, forecast_horizon, contain_missing_values, contain_equal_length = convert_tsf_to_dataframe(\"data_monash/kdd_cup_2018_dataset_without_missing_values.tsf\")         # DONE\n",
    "#loaded_data, frequency, forecast_horizon, contain_missing_values, contain_equal_length = convert_tsf_to_dataframe(\"data_monash/pedestrian_counts_dataset.tsf\")                           # DONE\n",
    "\n",
    "#loaded_data, frequency, forecast_horizon, contain_missing_values, contain_equal_length = convert_tsf_to_dataframe(\"data_monash/weather_dataset.tsf\")                                     # no date\n",
    "#loaded_data, frequency, forecast_horizon, contain_missing_values, contain_equal_length = convert_tsf_to_dataframe(\"data_monash/dominick_dataset.tsf\")                                    # no date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_dict = {'half_hourly': '30min',\n",
    "                  'daily': 'D',\n",
    "                  '10_minutes': '10min',\n",
    "                  'hourly': 'H',\n",
    "                  'monthly': 'MS', # CUIDADO\n",
    "                  'minutely': 'T',\n",
    "                  'weekly': 'W-SUN'}  # CUIDADO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_time_series(unique_id, start, periods, values, freq):\n",
    "    total_dates = pd.date_range(start=start, periods=periods, freq=freq)\n",
    "    time_series = pd.DataFrame({'unique_id': unique_id, 'ds': total_dates, 'y': values})\n",
    "    return time_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data(loaded_data, frequency):\n",
    "    loaded_data['count'] = loaded_data['series_value'].apply(lambda x: len(x))\n",
    "    data = pd.concat([_create_time_series(row[1]['series_name'],\n",
    "                                          row[1]['start_timestamp'],\n",
    "                                          row[1]['count'],\n",
    "                                          row[1]['series_value'],\n",
    "                                          frequency) for row in loaded_data.iterrows()])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = parse_data(loaded_data=loaded_data, frequency=frequency_dict[frequency])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniques = complete_data.ts_name.unique()\n",
    "sample = np.random.choice(uniques, 1000)\n",
    "sample_data = complete_data[complete_data.ts_name.isin(sample)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sample_data[['ts_name','ds','y']]\n",
    "data.columns = ['unique_id','ds','y']\n",
    "data.to_csv('dominick.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_transfer = complete_data[['ts_name','ds','y']]\n",
    "wiki_transfer.columns = ['unique_id','ds','y']\n",
    "wiki_transfer.to_csv('wiki_daily.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuralforecast",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
